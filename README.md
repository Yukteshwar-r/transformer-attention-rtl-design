# transformer-attention-rtl-design
SystemVerilog RTL implementation of the Scaled Dot-Product Attention mechanism used in Transformer models. Computes Query, Key, and Value matrices, and the resulting Attention using SRAM-based memory interfaces. Includes full simulation, evaluation, and synthesis workflows.
